# Mixture of Experts (MoE) from Scratch

This repository implements a Mixture of Experts (MoE) model from scratch, showcasing the fundamentals of this powerful technique for enhancing model scalability and efficiency.

## ğŸš€ Features
- Custom-built MoE architecture with dynamic routing.
- Modular design for easy integration and experimentation.
- Comprehensive training pipeline with data loading, evaluation, and visualization tools.

## ğŸ“‚ Project Structure
```
moe-from-scratch/
â”œâ”€â”€ data/            # Sample datasets for testing
â”œâ”€â”€ src/             # Custom MoE model implementations
â”œâ”€â”€ main.py          # Training script for the MoE model
â”œâ”€â”€ inference.py     # Inference script for model evaluation
â”œâ”€â”€ requirements.txt # Dependencies
â””â”€â”€ README.md        # Project documentation
```

## ğŸ› ï¸ Installation
1. Clone the repository:
   ```bash
   git clone https://github.com/naot97/moe-from-scratch.git
   cd moe-from-scratch
   ```
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

## ğŸ“‹ Usage
### Downloading data
To download the TinyStories dataset:
```bash
mkdir -p data && cd data \
  && wget -q --show-progress https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories_all_data.tar.gz \
  && tar -xzf TinyStories_all_data.tar.gz && rm TinyStories_all_data.tar.gz
```

### Training the Model
```bash
python main.py --action train
```

### Custom Configuration
Modify the `config/config.yaml` file to adjust model parameters, data paths, and hyperparameters.

## ğŸ“ˆ Results
Results will be logged in the console, accompanied by visualizations of model performance over time. Example console output:
```bash
[INFO] Starting training with 50 epochs...
[INFO] Epoch 1/50 - Loss: 0.5243 - Accuracy: 82.3%
[INFO] Epoch 25/50 - Loss: 0.3125 - Accuracy: 90.1%
[INFO] Epoch 50/50 - Loss: 0.2051 - Accuracy: 94.7%
[INFO] Training complete.
```

## ğŸ§ª Experiments
- Varying the number of experts to analyze performance trade-offs.
- Exploring different gating mechanisms for improved routing efficiency.

## ğŸ§© Future Enhancements
- Integration with PyTorch Lightning for improved training scalability.
- Addition of Transformer-based MoE models.
- Enhanced visualization tools for interpretability.

## ğŸ¤ Contributing
Contributions are welcome! Please open an issue or submit a pull request to discuss your ideas.

## ğŸ“„ License
This project is licensed under the MIT License. See the `LICENSE` file for details.

## ğŸ“§ Contact
For questions or collaboration opportunities, feel free to reach out or open an issue.

